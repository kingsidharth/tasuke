---
description: Background Tasks & Celery Rules
globs: 
alwaysApply: false
---
# BACKGROUND TASKS & CELERY RULES
## Celery Configuration and Task Patterns
```
python# app/worker/__init__.py
from celery import Celery
from app.core.config import settings
from app.core.logging import logger

# Celery app configuration
celery_app = Celery(
    "tasuke_worker",
    broker=settings.REDIS_URL,
    backend=settings.REDIS_URL,
    include=[
        "app.worker.embeddings",
        "app.worker.cleanup", 
        "app.worker.notifications"
    ]
)
```

# Celery configuration
```
celery_app.conf.update(
    # Task routing
    task_routes={
        "app.worker.embeddings.*": {"queue": "embeddings"},
        "app.worker.cleanup.*": {"queue": "maintenance"},
        "app.worker.notifications.*": {"queue": "notifications"}
    },
    
    # Task execution
    task_serializer="json",
    result_serializer="json",
    accept_content=["json"],
    result_expires=3600,
    timezone="UTC",
    enable_utc=True,
    
    # Retry configuration
    task_default_retry_delay=60,
    task_max_retries=3,
    
    # Worker configuration
    worker_prefetch_multiplier=1,
    task_acks_late=True,
    worker_disable_rate_limits=True,
    
    # Monitoring
    task_send_sent_event=True,
    task_track_started=True,
)
```

# Task base class with logging
```
from celery import Task

class LoggedTask(Task):
    """Base task class with automatic logging."""
    
    def on_success(self, retval, task_id, args, kwargs):
        logger.info("Task completed successfully", task_id=task_id, task_name=self.name)
    
    def on_failure(self, exc, task_id, args, kwargs, einfo):
        logger.error("Task failed", task_id=task_id, task_name=self.name, error=str(exc))
    
    def on_retry(self, exc, task_id, args, kwargs, einfo):
        logger.warning("Task retrying", task_id=task_id, task_name=self.name, error=str(exc))
```

# Set default task base
```
celery_app.Task = LoggedTask

# app/worker/cleanup.py - Maintenance tasks
from celery import shared_task
from datetime import datetime, timedelta
from app.db.session import get_db
from app.db.models import RawNote, Thread, Message
import os
import shutil

@shared_task(bind=True)
async def daily_cleanup(self):
    """Daily maintenance tasks."""
    
    logger.info("Starting daily cleanup")
    
    try:
        # Clean old files
        await cleanup_old_files()
        
        # Clean old logs
        await cleanup_old_logs()
        
        # Archive completed threads
        await archive_old_threads()
        
        # Database maintenance
        await database_maintenance()
        
        logger.info("Daily cleanup completed successfully")
        
    except Exception as e:
        logger.error("Daily cleanup failed", error=str(e))
        raise

async def cleanup_old_files():
    """Remove files older than 90 days."""
    
    storage_path = Path("storage/uploads")
    cutoff_date = datetime.now() - timedelta(days=90)
    
    cleaned_count = 0
    cleaned_size = 0
    
    for file_path in storage_path.rglob("*"):
        if file_path.is_file():
            file_stat = file_path.stat()
            file_modified = datetime.fromtimestamp(file_stat.st_mtime)
            
            if file_modified < cutoff_date:
                file_size = file_stat.st_size
                file_path.unlink()
                cleaned_count += 1
                cleaned_size += file_size
    
    logger.info("File cleanup completed", files_removed=cleaned_count, bytes_freed=cleaned_size)

async def cleanup_old_logs():
    """Clean log files (handled by loguru, but verify)."""
    
    logs_path = Path("logs")
    cutoff_date = datetime.now() - timedelta(days=90)
    
    for log_file in logs_path.glob("*.log*"):
        if log_file.is_file():
            file_stat = log_file.stat()
            file_modified = datetime.fromtimestamp(file_stat.st_mtime)
            
            if file_modified < cutoff_date and not log_file.name.endswith(".gz"):
                # Only remove uncompressed old logs (compressed ones handled by loguru)
                log_file.unlink()

async def archive_old_threads():
    """Archive threads completed more than 30 days ago."""
    
    cutoff_date = datetime.utcnow() - timedelta(days=30)
    
    async with get_db() as db:
        # Find old completed threads
        old_threads = await db.execute(
            select(Thread).where(
                Thread.status == "success",
                Thread.completed_at < cutoff_date,
                Thread.status != "archived"
            )
        )
        
        archived_count = 0
        for thread in old_threads.scalars():
            thread.status = "archived"
            archived_count += 1
        
        await db.commit()
        logger.info("Threads archived", count=archived_count)

async def database_maintenance():
    """Run database maintenance tasks."""
    
    async with get_db() as db:
        # Update table statistics
        tables = ["tasks", "threads", "raw_notes", "messages", "projects"]
        
        for table in tables:
            await db.execute(text(f"ANALYZE {table}"))
        
        await db.commit()
        logger.info("Database analysis completed")
```

# Periodic task scheduling
```
from celery.schedules import crontab

celery_app.conf.beat_schedule = {
    # Daily cleanup at 2 AM
    "daily-cleanup": {
        "task": "app.worker.cleanup.daily_cleanup",
        "schedule": crontab(hour=2, minute=0),
    },
    
    # Weekly database optimization
    "weekly-db-maintenance": {
        "task": "app.worker.cleanup.weekly_maintenance",
        "schedule": crontab(hour=3, minute=0, day_of_week=0),  # Sunday 3 AM
    },
}
```