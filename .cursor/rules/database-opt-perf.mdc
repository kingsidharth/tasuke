---
description: Database Optimisation & Performance Rules
globs: 
alwaysApply: false
---
# DATABASE OPTIMIZATION & PERFORMANCE RULES

## Database Performance Configuration
```
python# app/db/session.py - Optimized database configuration
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession, async_sessionmaker
from sqlalchemy.pool import QueuePool
from app.core.config import settings

# Optimized engine configuration
engine = create_async_engine(
    settings.DATABASE_URL,
    echo=settings.DEBUG,
    
    # Connection pool settings
    pool_size=20,                    # Number of connections to maintain
    max_overflow=30,                 # Additional connections allowed
    pool_pre_ping=True,             # Validate connections before use
    pool_recycle=3600,              # Recycle connections after 1 hour
    poolclass=QueuePool,
    
    # Connection arguments for PostgreSQL
    connect_args={
        "command_timeout": 60,       # Query timeout
        "application_name": "tasuke_backend",
        "options": "-c timezone=UTC"
    }
)
```

# Indexes for performance
`from sqlalchemy import Index, text`

# Add indexes in models
```
class Task(SQLModel, table=True):
    # ... existing fields ...
    
    __table_args__ = (
        Index('ix_tasks_status_created_at', 'status', 'created_at'),
        Index('ix_tasks_assignee_status', 'assignee', 'status'),
        Index('ix_tasks_project_status', 'project_id', 'status'),
        Index('ix_tasks_description_hash', 'description_hash'),
        Index('ix_tasks_due_date', 'due_date'),
    )

class RawNote(SQLModel, table=True):
    # ... existing fields ...
    
    __table_args__ = (
        Index('ix_raw_notes_source_created', 'source', 'created_at'),
        Index('ix_raw_notes_author_created', 'author', 'created_at'),
        Index('ix_raw_notes_content_hash', 'content_hash'),
        # Vector similarity index for pgvector
        Index('ix_raw_notes_content_vector', 'content_vector', postgresql_using='ivfflat'),
    )
```

# Query optimization patterns
```
async def get_tasks_optimized(
    db: AsyncSession,
    status: Optional[str] = None,
    assignee: Optional[str] = None,
    project_id: Optional[int] = None,
    limit: int = 50,
    offset: int = 0
) -> List[Task]:
    """Optimized task query with proper indexing."""
    
    query = select(Task)
    
    # Use indexes efficiently
    if status:
        query = query.where(Task.status == status)
    if assignee:
        query = query.where(Task.assignee == assignee)
    if project_id:
        query = query.where(Task.project_id == project_id)
    
    # Always order by indexed column and limit results
    query = query.order_by(Task.created_at.desc()).limit(limit).offset(offset)
    
    result = await db.execute(query)
    return result.scalars().all()
```

# Vector similarity search optimization
```
async def semantic_search_notes(
    db: AsyncSession,
    query_text: str,
    limit: int = 10,
    similarity_threshold: float = 0.8
) -> List[RawNote]:
    """Optimized vector similarity search."""
    
    # Generate query embedding
    query_embedding = await generate_embedding(query_text)
    
    # Use pgvector operators for efficient search
    query = select(RawNote).where(
        RawNote.content_vector.cosine_distance(query_embedding) < (1 - similarity_threshold)
    ).order_by(
        RawNote.content_vector.cosine_distance(query_embedding)
    ).limit(limit)
    
    result = await db.execute(query)
    return result.scalars().all()
```

# Bulk operations for performance
```
async def bulk_create_tasks(db: AsyncSession, tasks_data: List[Dict]) -> List[Task]:
    """Efficiently create multiple tasks."""
    
    tasks = [Task(**task_data) for task_data in tasks_data]
    
    # Use bulk insert for better performance
    db.add_all(tasks)
    await db.commit()
    
    # Refresh to get generated IDs
    for task in tasks:
        await db.refresh(task)
    
    return tasks
```

# Database maintenance utilities
```
async def analyze_table_stats(db: AsyncSession, table_name: str):
    """Analyze table statistics for query optimization."""
    
    await db.execute(text(f"ANALYZE {table_name}"))
    await db.commit()
    
    # Get table statistics
    stats_query = text("""
        SELECT 
            schemaname,
            tablename,
            n_tup_ins,
            n_tup_upd,
            n_tup_del,
            n_live_tup,
            n_dead_tup,
            last_autovacuum,
            last_autoanalyze
        FROM pg_stat_user_tables 
        WHERE tablename = :table_name
    """)
    
    result = await db.execute(stats_query, {"table_name": table_name})
    return result.fetchone()
```